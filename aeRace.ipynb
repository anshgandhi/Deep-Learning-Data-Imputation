{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#                         #\n",
    "#   Pre-process Dataset   #\n",
    "#                         #\n",
    "###########################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "data = pd.read_csv(\"./../data/data_train.csv\",low_memory=False)\n",
    "data.drop(\"KEY\",axis=1,inplace=True)\n",
    "print(\"Dataset Loaded!\")\n",
    "\n",
    "compCols = pickle.load( open( \"./../pickle/compCols.p\", \"rb\" ) )\n",
    "\n",
    "dummifyCols = []\n",
    "for i in compCols:\n",
    "    if len(data[i].unique()) > 2:\n",
    "        dummifyCols.append(i)\n",
    "\n",
    "removeCols = [\"DISCWT\",\"TOTCHG\",\"TOTAL_DISC\"]\n",
    "for i in removeCols:\n",
    "    dummifyCols.remove(i)\n",
    "\n",
    "dummifyCols.remove(\"RACE\")\n",
    "dummifyCols.append(\"RACE\")\n",
    "\n",
    "data = data[compCols]\n",
    "raceCols = pd.get_dummies(data,columns=dummifyCols).columns\n",
    "data = pd.get_dummies(data,columns=dummifyCols)\n",
    "X = preprocessing.minmax_scale(data.as_matrix(),axis=0)\n",
    "print(\"Data: \"+str(X.shape))\n",
    "X_train, X_test = train_test_split(X,test_size=0.3)\n",
    "print(\"Train Data: \"+str(X_train.shape))\n",
    "print(\"Test Data: \"+str(X_test.shape))\n",
    "print(\"Split Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "#                         #\n",
    "#       Train Model       #\n",
    "#                         #\n",
    "###########################\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "#Data Corruption Percentage\n",
    "data_destroy = 0.1\n",
    "\n",
    "#Flag to enable writing weights to file. (dump weights FLAG)\n",
    "wrtAE = 0\n",
    "\n",
    "training_epochs = 100001\n",
    "batch_size = 2000\n",
    "n_input = X_train.shape[1]\n",
    "keep_prob = 0.9\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_input])\n",
    "mask = tf.placeholder(\"bool\", [None, n_input],name='MASK')\n",
    "\n",
    "\n",
    "# hidden layer settings\n",
    "n_hidden_1 =  300\n",
    "n_hidden_2 = int(n_hidden_1/2)\n",
    "n_hidden_3 = int(n_hidden_2/2)\n",
    "\n",
    "\n",
    "folder = \"AE-race\"\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'encoder_h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_3, n_hidden_2])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h3': tf.Variable(tf.random_normal([n_hidden_1, n_input]))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b3': tf.Variable(tf.random_normal([n_input]))\n",
    "}\n",
    "\n",
    "\n",
    "#read pre-calculated weights\n",
    "'''\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/eh1.p\", \"rb\"))),\n",
    "    'encoder_h2': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/eh2.p\", \"rb\"))),\n",
    "    'encoder_h3': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/eh3.p\", \"rb\"))),\n",
    "    'decoder_h1': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/dh1.p\", \"rb\"))),\n",
    "    'decoder_h2': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/dh2.p\", \"rb\"))),\n",
    "    'decoder_h3': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/dh3.p\", \"rb\")))\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/eb1.p\", \"rb\"))),\n",
    "    'encoder_b2': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/eb2.p\", \"rb\"))),\n",
    "    'encoder_b3': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/eb3.p\", \"rb\"))),\n",
    "    'decoder_b1': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/db1.p\", \"rb\"))),\n",
    "    'decoder_b2': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/db2.p\", \"rb\"))),\n",
    "    'decoder_b3': tf.Variable(pickle.load(open(\"./../weights/\"+folder+\"/db3.p\", \"rb\")))\n",
    "}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    \n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    \n",
    "    layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['encoder_h3']),\n",
    "                                   biases['encoder_b3']))\n",
    "    return layer_3\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    \n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    \n",
    "    layer_2 = tf.nn.dropout(layer_2, keep_prob)\n",
    "    \n",
    "    layer_3 = tf.nn.sigmoid(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
    "                                   biases['decoder_b3']))\n",
    "    return layer_3\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(x)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = tf.boolean_mask(decoder_op, mask,name='boolean_mask1')\n",
    "\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = tf.boolean_mask(y, mask,name='boolean_mask2')\n",
    "\n",
    "# Define loss and optimizer, minimize the Cross Entropy\n",
    "y_pred = tf.reshape(y_pred, [-1, 6]) \n",
    "y_true = tf.reshape(y_true, [-1, 6]) \n",
    "cost = -tf.reduce_mean(y_true*tf.log(y_pred+1e-9) + (1-y_true)*tf.log(1-y_pred+1e-9))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.0001).minimize(cost)\n",
    "\n",
    "ar1 = tf.argmax(y_pred,1,name=\"arg1\")\n",
    "ar2 = tf.argmax(y_true,1,name=\"arg2\")\n",
    "correct_prediction = tf.equal(ar1,ar2)\n",
    "\n",
    "#Accuracy\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#Confusion Matrix\n",
    "confMat = tf.contrib.metrics.confusion_matrix(ar2,ar1,weights=None)\n",
    "\n",
    "#Initialize the variables\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "indices_train = np.random.choice(np.arange(batch_size),size=int(batch_size*data_destroy))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    total_batch = int(len(X_train)/batch_size)\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        #After every 1000 epochs train on a different Corrupted data\n",
    "        if epoch %1000 == 0:\n",
    "            global indices_train\n",
    "            indices_train = np.random.choice(np.arange(batch_size),size=int(batch_size*data_destroy))\n",
    "            \n",
    "        for i in range(total_batch):\n",
    "            batch_xs = X_train[i*batch_size:(i+1)*batch_size].copy()\n",
    "            batch_xs[indices_train,327:333] = 0\n",
    "            m = np.full((batch_size,6), False, dtype=bool)\n",
    "            m[indices_train,:] = True\n",
    "            a = np.full((batch_size,327), False, dtype=bool)\n",
    "            maskArr = np.concatenate((a,m), axis=1)\n",
    "            sess.run([optimizer],\n",
    "                     feed_dict={\n",
    "                         x: batch_xs,\n",
    "                         y:X_train[i*batch_size:(i+1)*batch_size],\n",
    "                         mask: maskArr\n",
    "                     })\n",
    "        \n",
    "        #Print Accuracy for every 2nd Epoch\n",
    "        if  epoch%2==0:\n",
    "            test_batch = X_test.copy()\n",
    "            indices = np.random.choice(np.arange(len(test_batch)),size=int(len(test_batch)*data_destroy))\n",
    "            test_batch[indices,327:333] = 0\n",
    "            m = np.full((len(test_batch),6), False, dtype=bool)\n",
    "            m[indices,:] = True\n",
    "            a = np.full((len(test_batch),327), False, dtype=bool)\n",
    "            maskArr = np.concatenate((a,m), axis=1)\n",
    "            test_batch[maskArr] = 0\n",
    "            curr_accuracy,confM = sess.run([accuracy,confMat],feed_dict={x: test_batch,\n",
    "                                                       y:X_test,\n",
    "                                                       mask: maskArr}\n",
    "                                           )\n",
    "            #Print Accuracy and Confusion Matrix\n",
    "            #print(\"Epoch-\"+str(epoch)+\" \"+str(curr_accuracy))\n",
    "            #print(confM)\n",
    "            \n",
    "            #Write the Accuracy with Epoch to a file to be visualised later on.\n",
    "            #with open(\"./../results/epochs-\"+folder+\".txt\", \"a\") as f:\n",
    "            #    f.write(str(epoch)+\",\"+str(round(100*curr_accuracy,3))+\"\\n\")\n",
    "            \n",
    "            #if wrtAE is set to 1, only then dump weights to file.\n",
    "            if (wrtAE == 1):\n",
    "                pickle.dump(sess.run(weights['encoder_h1']), open(\"./../weights/\"+folder+\"/eh1.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(weights['encoder_h2']), open(\"./../weights/\"+folder+\"/eh2.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(weights['encoder_h3']), open(\"./../weights/\"+folder+\"/eh3.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(weights['decoder_h1']), open(\"./../weights/\"+folder+\"/dh1.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(weights['decoder_h2']), open(\"./../weights/\"+folder+\"/dh2.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(weights['decoder_h3']), open(\"./../weights/\"+folder+\"/dh3.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(biases['encoder_b1']), open(\"./../weights/\"+folder+\"/eb1.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(biases['encoder_b2']), open(\"./../weights/\"+folder+\"/eb2.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(biases['encoder_b3']), open(\"./../weights/\"+folder+\"/eb3.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(biases['decoder_b1']), open(\"./../weights/\"+folder+\"/db1.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(biases['decoder_b2']), open(\"./../weights/\"+folder+\"/db2.p\", \"wb\"))\n",
    "                pickle.dump(sess.run(biases['decoder_b3']), open(\"./../weights/\"+folder+\"/db3.p\", \"wb\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
